{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503d0c2f-4d70-4134-9b12-42b2026094f5",
   "metadata": {},
   "source": [
    "## Topic: Sustainable risk preferences on asset allocation: a higher order optimal portfolio study\n",
    "# THI DIEU HUONG NGUYEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556d16c-24af-468a-9ea6-e918ba641f0f",
   "metadata": {},
   "source": [
    "#### - Data is directly imported into Jupyternotebook by using Yahoo finance API, hence the code and data are in 1 single ipynb file. Use the option ‘Restart kernel and run all cell’ is suggested, in case ‘run all cell’ option does not work.\n",
    "#### - Paper and its supplementary file are submitted together with the code file.\n",
    "#### - Regarding methodology implemented in code. E.g: GARCH (1,1) and GO-GARCH, please read the methodology given in the paper and its supplementary file given by the authors.\n",
    "#### - All results in the presentation are taken from the finance_project_official.ipynb. Data in the tables are collected in the results cell manually,e.g: collecting results of GARCH (1,1), since copying the whole GARCH model results and paste into the presentation slides is not a good idea\n",
    "#### - Paper source: https://doi.org/10.1016/j.jbef.2024.100887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f096ad-9fb0-4f93-807d-5bbe725ca0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\ceced\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: arch in c:\\users\\ceced\\anaconda3\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from arch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from arch) (1.11.4)\n",
      "Requirement already satisfied: pandas>=1.4 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from arch) (2.1.4)\n",
      "Requirement already satisfied: statsmodels>=0.12 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from arch) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from pandas>=1.4->arch) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from statsmodels>=0.12->arch) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from statsmodels>=0.12->arch) (23.1)\n",
      "Requirement already satisfied: six in c:\\users\\ceced\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.12->arch) (1.16.0)\n",
      "Requirement already satisfied: xlrd in c:\\users\\ceced\\anaconda3\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis as scipy_kurtosis, skew as scipy_skew, jarque_bera\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, gaussian_kde, invgauss\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "!pip install arch\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from scipy.optimize import minimize\n",
    "!pip install xlrd\n",
    "!pip install pandas openpyxl\n",
    "from sklearn.decomposition import FastICA\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy.linalg import eigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c281d-e781-40a3-92c1-57741370229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "print(yf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13755-2fd4-4f1f-94fe-8c51cf41c530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b282f-41ad-4b1f-aabf-c2ec943dbaba",
   "metadata": {},
   "source": [
    "# load data and calculate log return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67972e93-1577-4516-9d27-75e1b881b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IESE.AS belong to euro area is not included \n",
    "sri_ticker = [ \"PBW\",\"CXSE\",\"FAN\"] \n",
    "sri_data = yf.download(sri_ticker,start=\"2013-01-01\", end=\"2017-12-31\")\n",
    "sri_datatest = yf.download(sri_ticker,start=\"2018-01-01\", end=\"2019-12-31\")\n",
    "sri_data_full = yf.download(sri_ticker,start=\"2013-01-01\", end=\"2019-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93210c9a-ab9c-4c23-8ce5-5aa30d022642",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sri in sri_data:\n",
    "    sri_log_returns = np.log(sri_data['Close'] / sri_data['Close'].shift(1))\n",
    "for sri in sri_datatest:\n",
    "    sri_log_returnstest = np.log(sri_datatest['Close'] / sri_datatest['Close'].shift(1))\n",
    "for sri in sri_data_full:\n",
    "    sri_data_full_returns = np.log(sri_data_full['Close'] / sri_data_full['Close'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43995b74-2476-40d4-a6a9-ce84d5564d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAE belong to euro area is not included \n",
    "traditional_ticker = ['URTH', 'SPY','EWH'] \n",
    "traditional_data = yf.download(traditional_ticker,start=\"2013-01-01\", end=\"2017-12-31\")\n",
    "traditional_datatest = yf.download(traditional_ticker,start=\"2018-01-01\", end=\"2019-12-31\")\n",
    "traditional_data_full = yf.download(traditional_ticker,start=\"2013-01-01\", end=\"2019-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b58393-e7fc-45bd-b994-ca91788ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tra in traditional_data:\n",
    "    tra_log_returns = np.log(traditional_data['Close'] / traditional_data['Close'].shift(1))\n",
    "    \n",
    "for tra in traditional_datatest:\n",
    "    tra_log_returnstest = np.log(traditional_datatest['Close'] / traditional_datatest['Close'].shift(1))\n",
    "    \n",
    "for tra in traditional_data_full:\n",
    "    traditional_data_full_returns = np.log(traditional_data_full['Close'] / traditional_data_full['Close'].shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d7031-6cc5-4e7c-994d-6fb6f4b140c9",
   "metadata": {},
   "source": [
    "# descriptive statistic of log returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de027b-6f92-4d16-9db5-def264b5e019",
   "metadata": {},
   "source": [
    "### SUSTAINABLE(SRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f27d1d-0362-47f6-b634-13ae4a6d73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sri_log_returns=pd.DataFrame(sri_log_returns).dropna()\n",
    "sri_log_returns.index = pd.date_range(start=sri_log_returns.index[0], periods=len(sri_log_returns), freq='B')\n",
    "\n",
    "\n",
    "sri_log_returnstest=pd.DataFrame(sri_log_returnstest).dropna()\n",
    "sri_log_returnstest.index = pd.date_range(start=sri_log_returnstest.index[0], periods=len(sri_log_returnstest), freq='B')\n",
    "\n",
    "sri_data_full_returns=pd.DataFrame(sri_data_full_returns).dropna()\n",
    "sri_data_full_returns.index = pd.date_range(start=sri_data_full_returns.index[0], periods=len(sri_data_full_returns), freq='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836406e3-dbbe-4db6-a4b8-4dda99d0fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sri_log_returns))\n",
    "print(len(sri_data_full_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07e81b-8abb-470a-b8d9-5a5057320d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of sri in sample returns\n",
    "print(sri_log_returns.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852bfdf4-919f-44be-8fc6-f04e26551bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis\n",
    "skewness_sri = sri_log_returns.skew()\n",
    "kurtosis_sri = sri_log_returns.kurtosis()\n",
    "print(\"Skewness:\\n\", skewness_sri)\n",
    "print(\"Kurtosis:\\n\", kurtosis_sri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684b7e7-81e6-4d4e-992b-1bc47b8a565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_sri = sri_log_returns.corr()\n",
    "print(\"Correlation Matrix:\\n\", correlation_matrix_sri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68b722-a369-4d03-9e8f-64d11c0ef374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sri_ticker = [ 'PBW','CXSE','FAN'] \n",
    "plt.figure(figsize=(10, 5))\n",
    "# First subplot for 'URTH'\n",
    "plt.subplot(3, 1, 1)  # 3 rows, 1 column, 1st subplot\n",
    "plt.plot(sri_log_returns['CXSE'], label='CXSE ')\n",
    "plt.title('CXSE Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Second subplot for 'SPY'\n",
    "plt.subplot(3, 1, 2)  # 3 rows, 1 column, 2nd subplot\n",
    "plt.plot(sri_log_returns['FAN'], label='FAN ', color='red')\n",
    "plt.title('FAN Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Third subplot for 'EWH'\n",
    "plt.subplot(3, 1, 3)  # 3 rows, 1 column, 3rd subplot\n",
    "plt.plot(sri_log_returns['PBW'], label='PBW', color='green')\n",
    "plt.title('PBW Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d5cc6-cbcf-48e8-9078-19caad911c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'red', 'green']  # Adjusted color order to your specification\n",
    "\n",
    "plt.figure(figsize=(6, 8))  # Adjust the figure size to accommodate multiple subplots\n",
    "\n",
    "# Loop through each column and create a separate subplot for each\n",
    "for i, column in enumerate(sri_log_returns.columns):\n",
    "    plt.subplot(len(sri_log_returns.columns), 1, i + 1)  # Create a subplot for each asset\n",
    "    plt.hist(sri_log_returns[column], bins=20, alpha=0.75, color=colors[i], label=column)\n",
    "    plt.title(f'Histogram of Returns for {column}')\n",
    "    plt.xlabel('Returns')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap of subplot elements\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b804dd-1e7d-4e32-bbf6-2454c7a0e4d8",
   "metadata": {},
   "source": [
    "### TRADITIONAL(TRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16983e6-a122-4589-a14d-0fc935a46a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_log_returns=pd.DataFrame(tra_log_returns).dropna()\n",
    "tra_log_returns.index = pd.date_range(start=tra_log_returns.index[0], periods=len(tra_log_returns), freq='B')\n",
    "\n",
    "tra_log_returnstest=pd.DataFrame(tra_log_returnstest).dropna()\n",
    "tra_log_returnstest.index = pd.date_range(start=tra_log_returnstest.index[0], periods=len(tra_log_returnstest), freq='B')\n",
    "\n",
    "traditional_data_full_returns=pd.DataFrame(traditional_data_full_returns).dropna()\n",
    "traditional_data_full_returns.index = pd.date_range(start=traditional_data_full_returns.index[0], periods=len(traditional_data_full_returns), freq='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e456c2-d376-43ba-8895-04b08ea36ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tra_log_returns))\n",
    "print(len(traditional_data_full_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d14ba5-a235-476a-8169-f61f845ad7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of tra in sample returns\n",
    "print(tra_log_returns.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18df9c8-56ad-48f8-ac3b-3c3c3919597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis\n",
    "skewness = tra_log_returns.skew()\n",
    "kurtosis = tra_log_returns.kurtosis()\n",
    "print(\"Skewness:\\n\", skewness)\n",
    "print(\"Kurtosis:\\n\", kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989bd32-4993-4bab-b6e4-271c180f1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_tra = tra_log_returns.corr()\n",
    "print(\"Correlation Matrix:\\n\", correlation_matrix_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbbbcc-c717-4f11-bc0b-a12f01f55b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traditional_ticker = ['EWH', 'SPY','URTH'] \n",
    "plt.figure(figsize=(10, 5))\n",
    "# First subplot for 'URTH'\n",
    "plt.subplot(3, 1, 1)  # 3 rows, 1 column, 1st subplot\n",
    "plt.plot(tra_log_returns['EWH'], label='EWH')\n",
    "plt.title('URTH Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Second subplot for 'SPY'\n",
    "plt.subplot(3, 1, 2)  # 3 rows, 1 column, 2nd subplot\n",
    "plt.plot(tra_log_returns['SPY'], label='SPY', color='red')\n",
    "plt.title('SPY Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Third subplot for 'EWH'\n",
    "plt.subplot(3, 1, 3)  # 3 rows, 1 column, 3rd subplot\n",
    "plt.plot(tra_log_returns['URTH'], label='URTH', color='green')\n",
    "plt.title('EWH Log Returns')\n",
    "plt.legend()  # Ensure the legend is called right after plotting\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b535a-42c2-49d8-b963-41d2fefe5560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'red', 'green']  # Adjusted color order to your specification\n",
    "\n",
    "plt.figure(figsize=(6, 8\n",
    "                   ))  # Adjust the figure size to accommodate multiple subplots\n",
    "\n",
    "# Loop through each column and create a separate subplot for each\n",
    "for i, column in enumerate(tra_log_returns.columns):\n",
    "    plt.subplot(len(tra_log_returns.columns), 1, i + 1)  # Create a subplot for each asset\n",
    "    plt.hist(tra_log_returns[column], bins=20, alpha=0.75, color=colors[i], label=column)\n",
    "    plt.title(f'Histogram of Returns for {column}')\n",
    "    plt.xlabel('Returns')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap of subplot elements\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6e6b0-865c-43a5-bf8d-8f7670aee8ce",
   "metadata": {},
   "source": [
    "# GARCh (1,1) for each ETF in 2 Portfolio SRI & TRADITIONAL\n",
    "# Conditional variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7399e8-2418-42a4-a37a-36b0eafa32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate GARCH(1,1)\n",
    "def fit_garch_model(log_returns, etf_name):\n",
    "    # Initialize the GARCH 1,1 model\n",
    "    model = arch_model(log_returns, mean='Zero', vol='GARCH', p=1, q=1, rescale=False)\n",
    "    # Fit the model\n",
    "    model_fit = model.fit(disp='off')\n",
    "    # Print the summary of the model\n",
    "    print(f\"GARCH(1,1) Model Summary for {etf_name}:\")\n",
    "    print( model_fit.summary())\n",
    "    #print(f'conditional variance for {etf_name}:')\n",
    "    conditional_variances= model_fit.conditional_volatility**2\n",
    "    print('======')\n",
    "    return conditional_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f0cad-b447-4dea-84dd-974313d2a0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sri_ticker = [ 'PBW','CXSE','FAN']\n",
    "# GARCH (1,1) in sample\n",
    "conditional_variances_pbw=fit_garch_model(sri_log_returns['PBW'], 'PBW')\n",
    "conditional_variances_cxse=fit_garch_model(sri_log_returns['CXSE'], 'CXSE')\n",
    "conditional_variances_fan=fit_garch_model(sri_log_returns['FAN'], 'FAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8dfd4-6d6a-4af0-988c-606887a5f76d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sri_ticker = [ 'PBW','CXSE','FAN']\n",
    "# GARCH (1,1) out-of sample\n",
    "print('out of sample sri')\n",
    "conditional_variances_pbw_test=fit_garch_model(sri_log_returnstest['PBW'], 'PBW')\n",
    "conditional_variances_cxse_test=fit_garch_model(sri_log_returnstest['CXSE'], 'CXSE')\n",
    "conditional_variances_fan_test=fit_garch_model(sri_log_returnstest['FAN'], 'FAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863a290-e021-4cd8-bad8-27e6a41fd29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GARCH (1,1) in sample\n",
    "# traditional_ticker = ['URTH', 'SPY','EWH']\n",
    "conditional_variances_urth=fit_garch_model(tra_log_returns['URTH'], 'URTH')\n",
    "conditional_variances_spy=fit_garch_model(tra_log_returns['SPY'], 'SPY')\n",
    "conditional_variances_ewh=fit_garch_model(tra_log_returns['EWH'], 'EWH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d2304-7f16-4874-880e-92241983265d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GARCH (1,1) out of sample\n",
    "# traditional_ticker = ['URTH', 'SPY','EWH']\n",
    "conditional_variances_urth_test=fit_garch_model(tra_log_returnstest['URTH'], 'URTH')\n",
    "conditional_variances_spy_test=fit_garch_model(tra_log_returnstest['SPY'], 'SPY')\n",
    "conditional_variances_ewh_test=fit_garch_model(tra_log_returnstest['EWH'], 'EWH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64920d-6337-4b78-ba0a-5b5d88478ddd",
   "metadata": {},
   "source": [
    "# estimate lamda for each ETF in the sample period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6b046-c496-4aab-b864-3e763d5273c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_regression(data, dependent_var, conditional_variances):\n",
    "    # Create a temporary copy of data for manipulations to avoid altering the original DataFrame\n",
    "    temp_data = data.copy()\n",
    "    # Calculate 1-lagged return for the independent variable and store in the temporary DataFrame\n",
    "    temp_data['lagged_return'] = temp_data[dependent_var].shift(1)\n",
    "    # Attach the pre-computed conditional variances to the temporary DataFrame\n",
    "    temp_data['conditional_variances'] = conditional_variances\n",
    "    # Ensure there are no NaN values by dropping rows with NaNs in the newly created columns and the dependent variable\n",
    "    temp_data.dropna(subset=['lagged_return', 'conditional_variances', dependent_var], inplace=True)\n",
    "    \n",
    "    # Prepare independent variables (including constant for intercept)\n",
    "    X = sm.add_constant(temp_data[['lagged_return', 'conditional_variances']])\n",
    "    \n",
    "    # Define the dependent variable using the cleaned temporary DataFrame\n",
    "    y = temp_data[dependent_var]\n",
    "    \n",
    "    # Fit the OLS regression model\n",
    "    model_fit = sm.OLS(y, X).fit()\n",
    "    lambda_estimate = model_fit.params.get('conditional_variances', None)\n",
    "    print(f\"Estimated λ-coefficient for {dependent_var}: {lambda_estimate}\")\n",
    "    print(model_fit.summary())\n",
    "    print('***')\n",
    "    return model_fit\n",
    "    return lambda_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e50c6-23d4-43c9-ad96-97648ed7a39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sri_ticker = [ 'PBW','CXSE','FAN']\n",
    "# regression of in sample\n",
    "rraPBW = ols_regression(\n",
    "    data=sri_log_returns,\n",
    "    dependent_var='PBW',\n",
    "    conditional_variances=conditional_variances_pbw)\n",
    "rraCXSE = ols_regression(\n",
    "    data=sri_log_returns,\n",
    "    dependent_var='CXSE',\n",
    "    conditional_variances=conditional_variances_cxse)\n",
    "rraFAN = ols_regression(\n",
    "    data=sri_log_returns,\n",
    "    dependent_var='FAN',\n",
    "    conditional_variances=conditional_variances_fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160eb7c-865f-4880-b003-15ef811852b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traditional_ticker = ['URTH', 'SPY','EWH']\n",
    "# regression in sample\n",
    "rraURTH = ols_regression(\n",
    "    data=tra_log_returns,\n",
    "    dependent_var='URTH',\n",
    "    conditional_variances=conditional_variances_urth)\n",
    "rraSPY = ols_regression(\n",
    "    data=tra_log_returns,\n",
    "    dependent_var='SPY',\n",
    "    conditional_variances=conditional_variances_spy)\n",
    "rraEWH = ols_regression(\n",
    "    data=tra_log_returns,\n",
    "    dependent_var='EWH',\n",
    "    conditional_variances=conditional_variances_ewh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443d698-ae76-4e4a-b0ba-44677dfea943",
   "metadata": {},
   "source": [
    "# forecast lamda of FAN & PSY in out of sample period by choose the fit model, lamda of FAN & SPY to estimate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c0d00-c9e6-4e77-94c6-f17dbeec8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_lambda(data, dependent_var, conditional_variances, model_fit):\n",
    "    # Prepare the new predictor data\n",
    "    new_data = data.copy()\n",
    "    new_data['lagged_return'] = new_data[dependent_var].shift(1)  # Assuming 'data' is a DataFrame with log returns as its main column\n",
    "    new_data['conditional_variances'] = pd.Series(conditional_variances, index=new_data.index)\n",
    "    # Drop rows with NaN values which might occur due to shifting\n",
    "    new_data.dropna(inplace=True)\n",
    "    # Prepare the X matrix for prediction\n",
    "    X_new = sm.add_constant(new_data[['lagged_return', 'conditional_variances']])\n",
    "    # Predict new lambda values using the fitted model\n",
    "    new_data['predicted_lambda'] = model_fit.predict(X_new)\n",
    "    return new_data[['predicted_lambda']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b603f2-d93c-4560-94fe-a7804783fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sri_log_returnstest\n",
    "# rraFAN\n",
    "predicted_lambdas_fan = forecast_lambda(data=sri_log_returnstest, dependent_var='FAN', conditional_variances= conditional_variances_fan_test, model_fit= rraFAN)\n",
    "print(predicted_lambdas_fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedef6f-d35a-49ed-8535-09f5ec222780",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(predicted_lambdas_fan, label='fan')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77b0f7-578b-4294-b145-3a2737db076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_lambdas_fan['predicted_lambda'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f3f29-0529-467c-a0b5-f946a93191b0",
   "metadata": {},
   "source": [
    "### TRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f8dac-517c-4467-a2aa-1a5dd0d906a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tra_log_returnstest\n",
    "# rraSPY\n",
    "predicted_lambdas_spy = forecast_lambda(data=tra_log_returnstest, dependent_var='SPY', conditional_variances= conditional_variances_spy_test, model_fit= rraFAN)\n",
    "print(predicted_lambdas_spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64797e3c-0026-4ca5-abe6-d1094fce2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(predicted_lambdas_spy, label='spy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee175f8-8df8-4277-8c71-f9cf671458b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_lambdas_spy['predicted_lambda'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5783a3-f047-4d17-bd37-866e2841be5d",
   "metadata": {},
   "source": [
    "# go garch (1,1) for all ETFS in 2 portfolio direclty in the out of sample period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad86d9f-4c53-4fb2-a3ef-5b409c921a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to infer the frequency of the date index\n",
    "inferred_freq = pd.infer_freq(sri_log_returnstest.index)\n",
    "if inferred_freq is not None:\n",
    "    sri_log_returnstest.index.freq = inferred_freq\n",
    "\n",
    "inferred_freq = pd.infer_freq(tra_log_returnstest.index)\n",
    "if inferred_freq is not None:\n",
    "    tra_log_returnstest.index.freq = inferred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b45e02-3681-4395-bda0-2ac7bb73083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle NaN value\n",
    "print(\"NaNs in each column:\")\n",
    "print(sri_log_returnstest.isna().sum())\n",
    "# Check for infinite values\n",
    "print(\"\\nInfinite values in each column:\")\n",
    "print((np.isinf(sri_log_returnstest)).sum())\n",
    "sri_log_returnstest.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfe33-c7e8-4302-a222-4865e4a9586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle NaN value\n",
    "print(\"NaNs in each column:\")\n",
    "print(tra_log_returnstest.isna().sum())\n",
    "# Check for infinite values\n",
    "print(\"\\nInfinite values in each column:\")\n",
    "print((np.isinf(tra_log_returnstest)).sum())\n",
    "tra_log_returnstest.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ef381-fecc-4561-97d2-bfad095fed76",
   "metadata": {},
   "source": [
    "### Fit AR(1) for each ETFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf7872-a0c4-4554-bf6b-df70fd47adbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SRI AR 1 for each ETF\n",
    "# Fit AR(1) model to each SRI log returns\n",
    "conditional_means = pd.DataFrame(index=sri_log_returnstest.index, columns=sri_log_returnstest.columns)\n",
    "residuals = pd.DataFrame(index=sri_log_returnstest.index, columns=sri_log_returnstest.columns)\n",
    "model_params = pd.DataFrame(index=['mean', 'theta', 'variance'], columns=sri_log_returnstest.columns)\n",
    "\n",
    "for etf in sri_log_returns.columns:\n",
    "    model = AutoReg(sri_log_returnstest[etf], lags=1, old_names=False)\n",
    "    fitted_model = model.fit()\n",
    "    conditional_means[etf] = fitted_model.fittedvalues\n",
    "    residuals[etf] = fitted_model.resid\n",
    "\n",
    "    # Store model parameters\n",
    "    model_params.at['mean', etf] = fitted_model.params.get('const', 0)  # Safe access with default\n",
    "    model_params.at['theta', etf] = fitted_model.params.get(etf + '.L1', 0)  # Safe access with default\n",
    "    model_params.at['variance', etf] = fitted_model.sigma2\n",
    "\n",
    "# Compute the sample covariance matrix Σ from the AR residuals\n",
    "sri_covariance_matrix = residuals.cov()\n",
    "sri_covariance_array = sri_covariance_matrix.values\n",
    "\n",
    "# Output results\n",
    "print(f\"Model Parameters: (Mean, Theta, Variance):\")\n",
    "print(model_params)\n",
    "print(f\"Residual:\")\n",
    "print(residuals)\n",
    "print(\"Sample Covariance Matrix:\")\n",
    "print(sri_covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4409a-e57f-463a-88b1-4a12da6624c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SRI AR 1 for each ETF\n",
    "# Fit AR(1) model to each TRA log returns\n",
    "# Initialization of storage for fitted values, residuals, and model parameters\n",
    "conditional_means_tra = pd.DataFrame(index=tra_log_returnstest.index, columns=tra_log_returnstest.columns)\n",
    "residuals_tra = pd.DataFrame(index=tra_log_returnstest.index, columns=tra_log_returnstest.columns)\n",
    "model_params_tra = pd.DataFrame(index=['mean', 'theta', 'variance'], columns=tra_log_returnstest.columns)\n",
    "\n",
    "# Fitting the AR(1) model\n",
    "for etf in tra_log_returnstest.columns:\n",
    "    model_tra = AutoReg(tra_log_returnstest[etf].dropna(), lags=1, old_names=False)\n",
    "    fitted_model_tra = model_tra.fit()\n",
    "    conditional_means_tra[etf] = fitted_model_tra.fittedvalues\n",
    "    residuals_tra[etf] = fitted_model_tra.resid\n",
    "\n",
    "    # Store model parameters\n",
    "    model_params_tra.at['mean', etf] = fitted_model_tra.params.get('const', 0)\n",
    "    model_params_tra.at['theta', etf] = fitted_model_tra.params.get('L1', 0)\n",
    "    model_params_tra.at['variance', etf] = fitted_model_tra.sigma2\n",
    "\n",
    "# Computing the sample covariance matrix from the AR residuals\n",
    "tra_covariance_matrix = residuals_tra.cov()\n",
    "tra_covariance_array = tra_covariance_matrix.values\n",
    "\n",
    "# Output results\n",
    "print(\"Model Parameters: (Mean, Theta, Variance):\")\n",
    "print(model_params_tra)\n",
    "print(\"\\nResiduals:\")\n",
    "print(residuals_tra)\n",
    "print(\"\\nSample Covariance Matrix:\")\n",
    "print(tra_covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec308807-f1da-4e90-9c2d-ebdb0930c527",
   "metadata": {},
   "source": [
    "### whitening data and decomposition to get the mixing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bde418-5b10-4f64-a4a0-076179538aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "# SRI\n",
    "# Eigenvalue decomposition of the provided covariance matrix \n",
    "eigenvalues, eigenvectors = eigh(sri_covariance_matrix)\n",
    "\n",
    "# Whitening transformation\n",
    "positive_eigenvalues = np.clip(eigenvalues, a_min=1e-10, a_max=None)\n",
    "D_inv_sqrt = np.diag(1.0 / np.sqrt(positive_eigenvalues))\n",
    "whitening_matrix = eigenvectors @ D_inv_sqrt @ eigenvectors.T\n",
    "residuals_np = residuals.values.astype(float)\n",
    "whitened_data_np = np.dot(residuals.values, whitening_matrix)\n",
    "whitened_data = residuals.values @ whitening_matrix\n",
    "\n",
    "#forward fill any remaining NaN values #backward fill any remaining NaN values\n",
    "whitened_data_df = pd.DataFrame(whitened_data, index=residuals.index, columns=residuals.columns)\n",
    "whitened_data_df.ffill(inplace=True)\n",
    "whitened_data_df.bfill(inplace=True)\n",
    "\n",
    "ica = FastICA(whiten=False)  # No need to whiten as data is already pre-whitened\n",
    "try:\n",
    "    independent_components = ica.fit_transform(whitened_data)\n",
    "    mixing_matrix = ica.mixing_\n",
    "    unmixing_matrix = np.linalg.inv(mixing_matrix)\n",
    "\n",
    "    print(\"Independent Components:\\n\", independent_components)\n",
    "    print(\"Mixing Matrix A:\\n\", mixing_matrix)\n",
    "    print(\"Unmixing Matrix W (A^(-1)):\\n\", unmixing_matrix)\n",
    "except ValueError as e:\n",
    "    print(\"Error during FastICA:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013d584-54f1-4cb9-924a-24e54712418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRA\n",
    "tra_covariance_matrix_np = tra_covariance_matrix.values.astype(float)\n",
    "eigenvalues, eigenvectors = eigh(tra_covariance_matrix_np)\n",
    "\n",
    "positive_eigenvalues = np.clip(eigenvalues, a_min=1e-10, a_max=None)\n",
    "D_inv_sqrt = np.diag(1.0 / np.sqrt(positive_eigenvalues))\n",
    "whitening_matrix = eigenvectors @ D_inv_sqrt @ eigenvectors.T\n",
    "\n",
    "whitened_data = residuals_tra.values @ whitening_matrix\n",
    "whitened_data_df = pd.DataFrame(whitened_data, index=residuals_tra.index, columns=residuals_tra.columns)\n",
    "whitened_data_df.ffill(inplace=True)\n",
    "whitened_data_df.bfill(inplace=True)\n",
    "\n",
    "ica = FastICA(whiten=False)  # Since data is already pre-whitened\n",
    "try:\n",
    "    independent_components_tra = ica.fit_transform(whitened_data_df)\n",
    "    mixing_matrix_tra = ica.mixing_\n",
    "    unmixing_matrix_tra = np.linalg.inv(mixing_matrix_tra)\n",
    "\n",
    "    print(\"Independent Components TRA:\\n\", independent_components_tra)\n",
    "    print(\"Mixing Matrix A TRA:\\n\", mixing_matrix_tra)\n",
    "    print(\"Unmixing Matrix W (A^(-1)) TRA:\\n\", unmixing_matrix_tra)\n",
    "except ValueError as e:\n",
    "    print(\"Error during FastICA:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147acca1-bd51-40c2-aed5-c2ccee7530ad",
   "metadata": {},
   "source": [
    "### finding the factorial GARCH (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc4aa4-f2fe-4b99-a550-d4742bf0ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 3 sri GARCH (1,1) \n",
    "column_names = [f\"Factor_{i+1}\" for i in range(independent_components.shape[1])]\n",
    "independent_components_df = pd.DataFrame(independent_components, columns=column_names)\n",
    "H_f_t = []\n",
    "# Dictionary to store models and results\n",
    "garch_models = {}\n",
    "results_summary = {}\n",
    "\n",
    "# Loop through each column in the DataFrame which represents a factor.\n",
    "for factor in independent_components_df.columns:\n",
    "    # Initialize and fit a GARCH(1,1) model to the factor data.\n",
    "    model = arch_model(independent_components_df[factor], mean='Zero', vol='Garch', p=1, q=1)\n",
    "    res = model.fit(update_freq=0, disp='off')  # Fit the model quietly without updating the console.\n",
    "    H_f_t.append(res.conditional_volatility**2)\n",
    "    # Store the fitted model and results in dictionaries for easy access later.\n",
    "    garch_models[factor] = res\n",
    "    results_summary[factor] = {\n",
    "        'omega': res.params.get('omega', float('nan')),  # Constant variance component\n",
    "        'alpha': res.params.get('alpha[1]', float('nan')),  # Response of volatility to shocks\n",
    "        'beta': res.params.get('beta[1]', float('nan')),  # Volatility persistence\n",
    "        'Log Likelihood': res.loglikelihood  # Model's log likelihood\n",
    "    }\n",
    "\n",
    "# Optionally, convert the results summary to a DataFrame for better visualization and analysis.\n",
    "results_df = pd.DataFrame(results_summary).T  # Transpose to have factors as rows and parameters as columns.\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144fb17-ca06-4ad9-88ed-c3751115e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 tra GARCH !! \n",
    "column_names = [f\"Factor_{i+1}\" for i in range(independent_components_tra.shape[1])]\n",
    "independent_components_df = pd.DataFrame(independent_components_tra, columns=column_names)\n",
    "\n",
    "# Dictionary to store models and results\n",
    "garch_models = {}\n",
    "results_summary = {}\n",
    "H_f_t_tra = []\n",
    "# Loop through each column in the DataFrame which represents a factor.\n",
    "for factor in independent_components_df.columns:\n",
    "    # Initialize and fit a GARCH(1,1) model to the factor data.\n",
    "    model = arch_model(independent_components_df[factor], mean='Zero', vol='Garch', p=1, q=1)\n",
    "    res = model.fit(update_freq=0, disp='off')  # Fit the model quietly without updating the console.\n",
    "    H_f_t_tra.append(res.conditional_volatility**2)\n",
    "    # Store the fitted model and results in dictionaries for easy access later.\n",
    "    garch_models[factor] = res\n",
    "    results_summary[factor] = {\n",
    "        'omega': res.params.get('omega', float('nan')),  # Constant variance component\n",
    "        'alpha': res.params.get('alpha[1]', float('nan')),  # Response of volatility to shocks\n",
    "        'beta': res.params.get('beta[1]', float('nan')),  # Volatility persistence\n",
    "        'Log Likelihood': res.loglikelihood  # Model's log likelihood\n",
    "    }\n",
    "\n",
    "# Optionally, convert the results summary to a DataFrame for better visualization and analysis.\n",
    "results_df_tra = pd.DataFrame(results_summary).T  # Transpose to have factors as rows and parameters as columns.\n",
    "print(results_df_tra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b106222-2880-4a2e-b3eb-a29a9e2d0a6f",
   "metadata": {},
   "source": [
    "### compute the conditional covariance matrices Σ_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d8844-6148-45bd-a505-465d058aa8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Compute the conditional covariance matrices Σ_t of SRI\n",
    "H_f_t = np.array(H_f_t).T  # Transpose to have time on rows and factors on columns\n",
    "conditional_covariances_sri = []\n",
    "for t in range(H_f_t.shape[0]):\n",
    "    # Construct the diagonal matrix of variances for time t\n",
    "    H_t = np.diag(H_f_t[t])\n",
    "    # Calculate the conditional covariance matrix Σ_t\n",
    "    Σ_t = mixing_matrix@ H_t @ mixing_matrix.T\n",
    "    conditional_covariances_sri.append(Σ_t)\n",
    "conditional_covariances_sri = np.array(conditional_covariances_sri)  # Optional: convert list to array for easier handling\n",
    "\n",
    "print(\"Example of Conditional Covariance Matrix Σ_t of SRI at time t=0:\")\n",
    "print(conditional_covariances_sri[0])\n",
    "print(conditional_covariances_sri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9841d4-1613-493c-92b5-bfba702a7614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Compute the conditional covariance matrices Σ_t of TRA\n",
    "H_f_t_tra = np.array(H_f_t_tra).T  # Transpose to have time on rows and factors on columns\n",
    "conditional_covariances_tra = []\n",
    "for t in range(H_f_t_tra.shape[0]):\n",
    "    # Construct the diagonal matrix of variances for time t\n",
    "    H_t_tra = np.diag(H_f_t_tra[t])\n",
    "    # Calculate the conditional covariance matrix Σ_t\n",
    "    Σ_t = mixing_matrix_tra@H_t_tra@ mixing_matrix_tra.T\n",
    "    conditional_covariances_tra.append(Σ_t)\n",
    "conditional_covariances_tra = np.array(conditional_covariances_tra)  # Optional: convert list to array for easier handling\n",
    "\n",
    "print(\"Example of Conditional Covariance Matrix Σ_t of TRA:\")\n",
    "print(conditional_covariances_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d29b9e-f582-4e42-bee5-baa1cd69a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of covariance matrix')\n",
    "print(conditional_covariances_tra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd5c32-0321-469f-a731-a64463ce30db",
   "metadata": {},
   "source": [
    "### estimate the dynamic third order skewness & fourth order kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59801a6-ce01-46a8-b861-bdd6d7875a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 for SRI\n",
    "f_t_sri = independent_components  # Should be an array of shape (time_points, n_components)\n",
    "n_components_sri = f_t_sri.shape[1]  # Number of components/factors\n",
    "# Initialize arrays to store co-moments\n",
    "M3_t_sri = np.zeros((n_components_sri, n_components_sri, n_components_sri))\n",
    "M4_t_sri = np.zeros((n_components_sri, n_components_sri, n_components_sri, n_components_sri))\n",
    "\n",
    "# Assume A is your mixing matrix\n",
    "A = mixing_matrix  # Ensure this is correctly defined and corresponds with f_t in shape\n",
    "\n",
    "# Compute higher-order co-moments directly in the loops\n",
    "for i in range(n_components_sri):\n",
    "    for j in range(n_components_sri):\n",
    "        for k in range(n_components_sri):\n",
    "            # Compute the third-order co-moment for each combination of factors\n",
    "            third_moment = np.mean(f_t_sri[:, i] * f_t_sri[:, j] * f_t_sri[:, k])\n",
    "            M3_t_sri[i, j, k] = np.sum(A[i, :] * A[j, :] * A[k, :] * third_moment)\n",
    "\n",
    "for i in range(n_components_sri):\n",
    "    for j in range(n_components_sri):\n",
    "        for k in range(n_components_sri):\n",
    "            for l in range(n_components_sri):\n",
    "                # Compute the fourth-order co-moment for each combination of factors\n",
    "                fourth_moment = np.mean(f_t_sri[:, i] * f_t_sri[:, j] * f_t_sri[:, k] * f_t_sri[:, l])\n",
    "                M4_t_sri[i, j, k, l] = np.sum(A[i, :] * A[j, :] * A[k, :] * A[l, :] * fourth_moment)\n",
    "# Print results\n",
    "print(\"Dynamic third-order co-moment array SRI, M3_t:\")\n",
    "print(M3_t_sri[1])  # Example of the third co-moment array for the second set of factors\n",
    "print(\"Dynamic fourth-order co-moment array SRI, M4_t:\")\n",
    "print(M4_t_sri[1])  # Example of the fourth co-moment array for the second set of factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4967f-db9a-4f3c-8a7d-349b6f7bd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 for TRA\n",
    "f_t = independent_components_tra  # Should be an array of shape (time_points, n_components)\n",
    "n_components = f_t.shape[1]  # Number of components/factors\n",
    "# Initialize arrays to store co-moments\n",
    "M3_t_tra = np.zeros((n_components, n_components, n_components))\n",
    "M4_t_tra = np.zeros((n_components, n_components, n_components, n_components))\n",
    "\n",
    "# Assume A is your mixing matrix\n",
    "A = mixing_matrix_tra  # Ensure this is correctly defined and corresponds with f_t in shape\n",
    "\n",
    "# Compute higher-order co-moments directly in the loops\n",
    "for i in range(n_components):\n",
    "    for j in range(n_components):\n",
    "        for k in range(n_components):\n",
    "            # Compute the third-order co-moment for each combination of factors\n",
    "            third_moment = np.mean(f_t[:, i] * f_t[:, j] * f_t[:, k])\n",
    "            M3_t_tra[i, j, k] = np.sum(A[i, :] * A[j, :] * A[k, :] * third_moment)\n",
    "\n",
    "for i in range(n_components):\n",
    "    for j in range(n_components):\n",
    "        for k in range(n_components):\n",
    "            for l in range(n_components):\n",
    "                # Compute the fourth-order co-moment for each combination of factors\n",
    "                fourth_moment = np.mean(f_t[:, i] * f_t[:, j] * f_t[:, k] * f_t[:, l])\n",
    "                M4_t_tra[i, j, k, l] = np.sum(A[i, :] * A[j, :] * A[k, :] * A[l, :] * fourth_moment)\n",
    "\n",
    "# Print results\n",
    "print(\"Dynamic third-order co-moment array TRA, M3_t:\")\n",
    "print(M3_t_tra[1])  # Example of the third co-moment array for the second set of factors\n",
    "print(\"Dynamic fourth-order co-moment array TRA, M4_t:\")\n",
    "print(M4_t_tra[1])  # Example of the fourth co-moment array for the second set of factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431ecda-8be4-455f-b6fa-8f0eae8d4b52",
   "metadata": {},
   "source": [
    "# portfolio optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f456d3-dca4-43db-a31a-bf3b9086e427",
   "metadata": {},
   "source": [
    "### SRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665dc91-7a45-4f26-9f8f-6a1f48e7f17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_lambdas_fan = pd.DataFrame(predicted_lambdas_fan,columns=['predicted_lambda'])\n",
    "predicted_lambdas_fan.index = pd.date_range(start=predicted_lambdas_fan.index[0], periods=len(predicted_lambdas_fan), freq='B')\n",
    "print(predicted_lambdas_fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203164e-78f8-4d63-b1e2-a3a6a17a75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M3_t_sri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afa3bf-31c5-4ae3-bff4-5e59844dc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M4_t_sri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befa936-eb33-4978-b1c3-59be67cab225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trimming conditional_covariances_tra to match the length of predicted_lambdas_spy\n",
    "conditional_covariances_sri = conditional_covariances_sri[:500]\n",
    "sri_ticker_1 = [ 'CXSE','FAN','PBW'] \n",
    "# Assuming M3_t_sri and M4_t_sri are static, we only need one set of these matrices\n",
    "assert M3_t_sri.shape == (3, 3, 3), f\"Expected skewness matrix shape (3, 3, 3) but got {M3_t_sri.shape}\"\n",
    "assert M4_t_sri.shape == (3, 3, 3, 3), f\"Expected kurtosis matrix shape (3, 3, 3, 3) but got {M4_t_sri.shape}\"\n",
    "\n",
    "# Risk exposure function\n",
    "def risk_exposure(weights, lambda_value, cov_matrix, skewness_matrix, kurtosis_matrix):\n",
    "    weights_tf = tf.constant(weights, dtype=tf.float32)\n",
    "    \n",
    "    # Portfolio variance\n",
    "    cov_matrix_tf = tf.constant(cov_matrix, dtype=tf.float32)\n",
    "    portfolio_variance = tf.tensordot(weights_tf, tf.tensordot(cov_matrix_tf, weights_tf, axes=1), axes=1)\n",
    "    \n",
    "    # Portfolio skewness\n",
    "    skewness_matrix_tf = tf.constant(skewness_matrix, dtype=tf.float32)\n",
    "    portfolio_skewness = tf.einsum('i,j,k,ijk->', weights_tf, weights_tf, weights_tf, skewness_matrix_tf)\n",
    "    \n",
    "    # Portfolio kurtosis\n",
    "    kurtosis_matrix_tf = tf.constant(kurtosis_matrix, dtype=tf.float32)\n",
    "    portfolio_kurtosis = tf.einsum('i,j,k,l,ijkl->', weights_tf, weights_tf, weights_tf, weights_tf, kurtosis_matrix_tf)\n",
    "    \n",
    "    # Calculate risk exposure\n",
    "    risk_exposure_value = (0.5 * lambda_value * portfolio_variance - \n",
    "                           (lambda_value**2 / 6) * portfolio_skewness + \n",
    "                           (lambda_value**3 / 24) * (portfolio_kurtosis - 3))\n",
    "    \n",
    "    # Ensure the return value is a scalar\n",
    "    return risk_exposure_value.numpy().item()\n",
    "\n",
    "# Constants and Data\n",
    "n_assets = 3  # Number of assets in the portfolio\n",
    "sri_ticker = ['PBW', 'CXSE', 'FAN']\n",
    "\n",
    "# Ensure the lambda series has the correct length\n",
    "assert len(predicted_lambdas_fan) == 500, f\"Expected 500 but got {len(predicted_lambdas_fan)}\"\n",
    "assert conditional_covariances_sri.shape == (500, 3, 3), f\"Expected (500, 3, 3) but got {conditional_covariances_sri.shape}\"\n",
    "\n",
    "# Bounds and constraints\n",
    "bounds = [(0, 1)] * n_assets\n",
    "cons = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}  # Sum of weights must be 1\n",
    "\n",
    "# Initialize DataFrame to store optimal weights with ticker names as columns\n",
    "optimal_weights_sri = pd.DataFrame(index=predicted_lambdas_fan.index, columns=sri_ticker_1)\n",
    "\n",
    "# Dynamic Optimization Over Time\n",
    "for idx, (date, lambda_value) in enumerate(predicted_lambdas_fan['predicted_lambda'].items()):\n",
    "    cov_matrix = conditional_covariances_sri[idx]\n",
    "    result = minimize(risk_exposure, [1/n_assets] * n_assets, \n",
    "                      args=(lambda_value, cov_matrix, M3_t_sri, M4_t_sri),\n",
    "                      method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    \n",
    "    # Debugging information\n",
    "    print(f\"Date: {date}, Success: {result.success}, Message: {result.message}, Weights: {result.x}\")\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights_sri.loc[date] = result.x\n",
    "    else:\n",
    "        optimal_weights_sri.loc[date] = [np.nan] * n_assets  # Handle failed optimization\n",
    "\n",
    "# Output results\n",
    "print(\"Optimal Weights Over Time SRI:\")\n",
    "print(optimal_weights_sri)\n",
    "\n",
    "# Descriptive statistics of the optimal weights\n",
    "print(\"Descriptive Statistics of Optimal Weights SRI:\")\n",
    "print(optimal_weights_sri.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466644c-98bb-4225-910a-ef2a91467c4c",
   "metadata": {},
   "source": [
    "### TRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01f4ca-a9e9-4d26-b928-a07cefd0b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lambdas_spy = pd.DataFrame(predicted_lambdas_spy,columns=['predicted_lambda'])\n",
    "predicted_lambdas_spy.index = pd.date_range(start=predicted_lambdas_spy.index[0], periods=len(predicted_lambdas_spy), freq='B')\n",
    "print(predicted_lambdas_spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd39a48-7c96-4e34-8b6b-d3b6adeba4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M3_t_tra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c293d09-101a-4981-9bf6-57d38e9e829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M4_t_tra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cf0a4-62d4-4922-9882-bd14b3da8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming conditional_covariances_tra to match the length of predicted_lambdas_spy\n",
    "conditional_covariances_tra = conditional_covariances_tra[:500]\n",
    "traditional_ticker = ['EWH','URTH', 'SPY']\n",
    "# Assuming M3_t_sri and M4_t_sri are static, we only need one set of these matrices\n",
    "assert M3_t_tra.shape == (3, 3, 3), f\"Expected skewness matrix shape (3, 3, 3) but got {M3_t_tra.shape}\"\n",
    "assert M4_t_tra.shape == (3, 3, 3, 3), f\"Expected kurtosis matrix shape (3, 3, 3, 3) but got {M4_t_tra.shape}\"\n",
    "\n",
    "# Risk exposure function\n",
    "def risk_exposure(weights, lambda_value, cov_matrix, skewness_matrix, kurtosis_matrix):\n",
    "    weights_tf = tf.constant(weights, dtype=tf.float32)\n",
    "    \n",
    "    # Portfolio variance\n",
    "    cov_matrix_tf = tf.constant(cov_matrix, dtype=tf.float32)\n",
    "    portfolio_variance = tf.tensordot(weights_tf, tf.tensordot(cov_matrix_tf, weights_tf, axes=1), axes=1)\n",
    "    \n",
    "    # Portfolio skewness\n",
    "    skewness_matrix_tf = tf.constant(skewness_matrix, dtype=tf.float32)\n",
    "    portfolio_skewness = tf.einsum('i,j,k,ijk->', weights_tf, weights_tf, weights_tf, skewness_matrix_tf)\n",
    "    \n",
    "    # Portfolio kurtosis\n",
    "    kurtosis_matrix_tf = tf.constant(kurtosis_matrix, dtype=tf.float32)\n",
    "    portfolio_kurtosis = tf.einsum('i,j,k,l,ijkl->', weights_tf, weights_tf, weights_tf, weights_tf, kurtosis_matrix_tf)\n",
    "    \n",
    "    # Calculate risk exposure\n",
    "    risk_exposure_value = (0.5 * lambda_value * portfolio_variance - \n",
    "                           (lambda_value**2 / 6) * portfolio_skewness + \n",
    "                           (lambda_value**3 / 24) * (portfolio_kurtosis - 3))\n",
    "    \n",
    "    # Ensure the return value is a scalar\n",
    "    return risk_exposure_value.numpy().item()\n",
    "\n",
    "# Constants and Data\n",
    "n_assets = 3  # Number of assets in the portfolio\n",
    "\n",
    "# Ensure the lambda series has the correct length\n",
    "assert len(predicted_lambdas_spy) == 500, f\"Expected 500 but got {len(predicted_lambdas_spy)}\"\n",
    "assert conditional_covariances_tra.shape == (500, 3, 3), f\"Expected (500, 3, 3) but got {conditional_covariances_tra.shape}\"\n",
    "\n",
    "# Bounds and constraints\n",
    "bounds = [(0, 1)] * n_assets\n",
    "cons = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}  # Sum of weights must be 1\n",
    "\n",
    "# Initialize DataFrame to store optimal weights with ticker names as columns\n",
    "optimal_weights_tra = pd.DataFrame(index=predicted_lambdas_fan.index, columns=traditional_ticker)\n",
    "\n",
    "# Dynamic Optimization Over Time\n",
    "for idx, (date, lambda_value) in enumerate(predicted_lambdas_spy['predicted_lambda'].items()):\n",
    "    cov_matrix = conditional_covariances_tra[idx]\n",
    "    result = minimize(risk_exposure, [1/n_assets] * n_assets, \n",
    "                      args=(lambda_value, cov_matrix, M3_t_tra, M4_t_tra),\n",
    "                      method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    \n",
    "    # Debugging information\n",
    "    print(f\"Date: {date}, Success: {result.success}, Message: {result.message}, Weights: {result.x}\")\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights_tra.loc[date] = result.x\n",
    "    else:\n",
    "        optimal_weights_tra.loc[date] = [np.nan] * n_assets  # Handle failed optimization\n",
    "\n",
    "# Output results\n",
    "print(\"Optimal Weights Over Time TRA:\")\n",
    "print(optimal_weights_tra)\n",
    "\n",
    "# Descriptive statistics of the optimal weights\n",
    "print(\"Descriptive Statistics of Optimal Weights TRA:\")\n",
    "print(optimal_weights_tra.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f763b-36ac-420b-ab8c-def619867203",
   "metadata": {},
   "source": [
    "# Portfolio return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf678b-75fa-4735-a3a0-cdd907978bd1",
   "metadata": {},
   "source": [
    "### SRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310d519-5301-4ab3-be5e-482dba3ea860",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sri_log_returnstest.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56310b89-c75a-40b2-8f5c-242d31754bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_weights_sri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57390534-7c1f-4819-b62c-ceb1d4943133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sri_log_returnstest.index = pd.to_datetime(sri_log_returnstest.index)\n",
    "\n",
    "# Print the initial DataFrame and its index to confirm\n",
    "print(\"Original DataFrame:\")\n",
    "print(sri_log_returnstest)\n",
    "print(sri_log_returnstest.index)\n",
    "\n",
    "# Trim the DataFrame starting from '2018-01-04'\n",
    "trim_log_returns = sri_log_returnstest.loc['2018-01-04':]\n",
    "\n",
    "# Print the trimmed DataFrame and its index to confirm\n",
    "print(\"\\nTrimmed DataFrame:\")\n",
    "print(trim_log_returns)\n",
    "print(trim_log_returns.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49554a97-187e-40ae-9ee9-35c73bf920d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns of trim_log_returns:\")\n",
    "print(trim_log_returns.columns)\n",
    "\n",
    "print(\"Columns of optimal_weights_sri:\")\n",
    "print(optimal_weights_sri.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b212f-ea4d-47c8-bbdf-daef3d5643a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the indices match\n",
    "optimal_weights_sri = optimal_weights_sri.reindex(trim_log_returns.index)\n",
    "\n",
    "# Fill any potential missing values (if any dates are missing in the weights data)\n",
    "optimal_weights_sri.ffill(inplace=True)\n",
    "\n",
    "# Remove the column name from trim_log_returns to match optimal_weights_sri\n",
    "trim_log_returns.columns.name = None\n",
    "\n",
    "# Rename and reorder the columns of optimal_weights_sri to match those of trim_log_returns\n",
    "optimal_weights_sri = optimal_weights_sri[trim_log_returns.columns]\n",
    "\n",
    "# Verify the column names after renaming and reordering\n",
    "print(\"Columns of optimal_weights_sri after renaming and reordering:\")\n",
    "print(optimal_weights_sri.columns)\n",
    "print(\"Columns of trim_log_returns:\")\n",
    "print(trim_log_returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41aa35c-52a3-4135-bb1e-a437a5da5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the indices match\n",
    "optimal_weights_sri = optimal_weights_sri.reindex(trim_log_returns.index)\n",
    "\n",
    "# Fill any potential missing values (if any dates are missing in the weights data)\n",
    "optimal_weights_sri.ffill(inplace=True)\n",
    "\n",
    "def calculate_portfolio_returns_from_weights(log_returns, optimal_weights):\n",
    "    # Check alignment of indices\n",
    "    if not log_returns.index.equals(optimal_weights.index):\n",
    "        raise ValueError(\"Indices of log returns and optimal weights must match.\")\n",
    "    \n",
    "    # Convert weights DataFrame from object to float if necessary\n",
    "    optimal_weights = optimal_weights.astype(float)\n",
    "    \n",
    "    # Ensure optimal weights and log returns have the same columns\n",
    "    if list(optimal_weights.columns) != list(log_returns.columns):\n",
    "        raise ValueError(\"Columns of optimal weights must match columns of log returns.\")\n",
    "    \n",
    "    # Compute the portfolio returns by element-wise multiplication and then sum along the columns\n",
    "    portfolio_returns_ = (log_returns * optimal_weights).sum(axis=1)\n",
    "    \n",
    "    return portfolio_returns_\n",
    "\n",
    "# Calculate the portfolio returns\n",
    "try:\n",
    "    sri_portfolio = calculate_portfolio_returns_from_weights(trim_log_returns, optimal_weights_sri)\n",
    "    print(sri_portfolio)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816532c-6888-4717-83f1-b2cd52a4c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_summarize(portfolio_returns):\n",
    "    # Ensure portfolio_returns is a one-dimensional Series\n",
    "    if isinstance(portfolio_returns, pd.DataFrame):\n",
    "        portfolio_returns = portfolio_returns.stack()\n",
    "    \n",
    "    # Calculate mean and variance for the entire series\n",
    "    mean_return = portfolio_returns.mean()\n",
    "    variance_return = portfolio_returns.var()\n",
    "    \n",
    "    # Calculate kurtosis and skewness using scipy.stats functions\n",
    "    kurt = scipy_kurtosis(portfolio_returns, fisher=False)\n",
    "    skewness = scipy_skew(portfolio_returns)\n",
    "    \n",
    "    # Calculate Jarque-Bera test\n",
    "    jb_stat, jb_p_value = jarque_bera(portfolio_returns)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Mean of Portfolio Returns: {mean_return}\")\n",
    "    print(f\"Variance of Portfolio Returns: {variance_return}\")\n",
    "    print(f\"Kurtosis of Portfolio Returns: {kurt}\")\n",
    "    print(f\"Skewness of Portfolio Returns: {skewness}\")\n",
    "    print(f\"Jarque-Bera Test Statistic: {jb_stat}\")\n",
    "    print(f\"Jarque-Bera Test p-value: {jb_p_value}\")\n",
    "\n",
    "# Summarize the portfolio returns\n",
    "portfolio_summarize(sri_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccb783-967d-4016-a11d-e1d8a2e907e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))  \n",
    "plt.hist(sri_portfolio, label='SRI')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a15979-004b-4ebe-8327-c5e1f2756b48",
   "metadata": {},
   "source": [
    "### TRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268be5f-703d-4fe4-9861-9577b7183e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tra_log_returnstest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac95595-2140-473a-95d6-f1ab7f7e5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_weights_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98f6b1-71b3-483f-8659-0f7be9ad63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_log_returnstest.index = pd.to_datetime(tra_log_returnstest.index)\n",
    "\n",
    "# Print the initial DataFrame and its index to confirm\n",
    "print(\"Original DataFrame:\")\n",
    "print(tra_log_returnstest)\n",
    "print(tra_log_returnstest.index)\n",
    "\n",
    "# Trim the DataFrame starting from '2018-01-04'\n",
    "trim_log_returns_tra = tra_log_returnstest.loc['2018-01-04':]\n",
    "\n",
    "# Print the trimmed DataFrame and its index to confirm\n",
    "print(\"\\nTrimmed DataFrame:\")\n",
    "print(trim_log_returns_tra)\n",
    "print(trim_log_returns_tra.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e0a73-5f5b-4b78-86b1-bfa8ffb15d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns of trim_log_returns tra:\")\n",
    "print(trim_log_returns_tra.columns)\n",
    "\n",
    "print(\"Columns of optimal_weights_tra:\")\n",
    "print(optimal_weights_tra.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304a180-7e06-432e-a00d-425cee0600ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the indices match\n",
    "optimal_weights_tra = optimal_weights_tra.reindex(trim_log_returns_tra.index)\n",
    "\n",
    "# Fill any potential missing values (if any dates are missing in the weights data)\n",
    "optimal_weights_tra.ffill(inplace=True)\n",
    "\n",
    "# Remove the column name from trim_log_returns to match optimal_weights_sri\n",
    "trim_log_returns_tra.columns.name = None\n",
    "\n",
    "# Rename and reorder the columns of optimal_weights_sri to match those of trim_log_returns\n",
    "optimal_weights_tra = optimal_weights_tra[trim_log_returns_tra.columns]\n",
    "\n",
    "# Verify the column names after renaming and reordering\n",
    "print(\"Columns of optimal_weights_tra after renaming and reordering:\")\n",
    "print(optimal_weights_tra.columns)\n",
    "print(\"Columns of trim_log_returns tra:\")\n",
    "print(optimal_weights_tra.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1055f6-e7ea-489d-a75a-ccc643884d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the indices match\n",
    "# Fill any potential missing values (if any dates are missing in the weights data)\n",
    "optimal_weights_tra.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e34106-ba73-48d5-b8eb-89ab5ee4e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_portfolio= calculate_portfolio_returns_from_weights(trim_log_returns_tra, optimal_weights_tra)\n",
    "print(tra_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844dda19-e257-49c5-adae-e8ef6d88abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_summarize(tra_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a0c41-3155-4d3e-87a4-52a931128859",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))  \n",
    "plt.hist(tra_portfolio, label='TRA', color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf842e-55d8-4814-9857-684e95ae2d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
